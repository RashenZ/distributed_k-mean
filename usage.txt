
At the beginning we wanted to do a DKM (Distributed K-Means) with real data that came from different meteo API (). We this method we would've been able to create cluster depending of the information we retrieved. For this we wanted to use those following library:
- networkx : Python package for the creation, manipulation, and study of the structure, dynamics, and functions of complex networks.
- pandas : open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language
- sklearn : Open-source ML (Machine Learning) library for Python. sklearn (Scikit-learn) is a library in Python that provides many unsupervised and supervised learning algorithms.

+But we realize after one week that we were still stuck on the usage of networkx and sklearn and also thinking about the structure of the code ... (setup a network of node, create the node and set they neighbour, set they centroid ..Etc) so we decided to give this idea up because of time. [NEED BULLSHIT FOR THIS PART AYAAAAAA]

At the end we created a script that generate raw data.

HOW OUR PROGRAM WORK ?
Usage of our first script called DataGenerator.py

The script will allow us to generate data for each node and store it into a file by passing some parameters

	- nb_node	= Number of node
	- nb_obs	= Number of observation generated randomly
	- min_val	= Minimum value of the data generated
	- max_val	= Maximum value of the data generated
example:
	0:1,10,5,2,9
	1:6,5,10,2,9
	2:9,10,1,8,7

Then use our second script called ParsingData.py

It will allow us to parse the previous file generated

We initialize our class DistributedKMeans by sending in parameters :
- self.Xm = Xm # Set of all observation of not m {node(x): [observation1, observation2]}
		- Xm 	= Set of all observation of not m {node(x): [observation1, observation2, ..., observation(n)]}
		- M 	= Number of node
		- K 	= Number of cluster
		- Nm  	= Number of observation of node m
		- T 	= Number max of iteration
If you don't send one of those parameters, a default value will be set:
		- Xm 	= {"0": [1, 2, 3, 15], "1": [2, 2, 5, 1], "2": [4, 3, 10, 50]}
		- M 	= 5
		- K 	= 3
		- Nm 	= 10
		- T 	= 5

Once everything is setup we call a method function called "start"
 $> KMeans.start()

 this function will set our cluster (depending on the data we have) by calling the method self.distributedVarPart().
 Through this function we'll 
 first : initialize our cluster
 then we'll set a centroid for each cluster
 and after calculate the SSE (Sum of Squares Error) also for each of them ( function calculateSSE(self, cluster, center) ) and depending of the result :
 	. If the SSE is too big the cluster will be split it in 2 sub-cluster (function splitCluster(self, cluster) ).
 	. Else we keep the cluster like this.

Now that we initialized our cluster and our centroid we can start our loop that will iterate and calculate the new centroid for each cluster.
After calculating the new centroid the program will display them as followed:

$> turn t=0: Centroids[10, 23, 14, 2, ..., x]
$> turn t=1: Centroids[10, 24, 13, 2, ..., x]
$> turn t=N: Centroids[a, b, c, d, ..., x]